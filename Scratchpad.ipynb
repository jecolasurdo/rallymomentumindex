{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BLM Keyword Identification\n",
    "Sandboxing some simple keyword identification routines.\n",
    "Full scale work will probably not be done in a notebook, mostly exploration here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import getcwd, listdir\n",
    "import os.path\n",
    "from concurrent.futures import ThreadPoolExecutor, wait\n",
    "from pickle import Pickler, Unpickler\n",
    "\n",
    "import spacy\n",
    "from scipy import sparse\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_lg\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gather a list of codex entries to analyze."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "DOC_LIMIT = 500\n",
    "MAX_WORKERS = None\n",
    "\n",
    "base_path = os.path.join(getcwd(), \"research/data/codex\")\n",
    "\n",
    "file_names = [f for f in listdir(base_path) if os.path.isfile(os.path.join(base_path, f)) and f.endswith(\".txt\")]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Identifing an algorithm for named entity extraction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_entities(text):\n",
    "    '''\n",
    "    Identify entities and noun_chunks within the document,\n",
    "    and filter them such that there are no overlapping spans.\n",
    "    Also taking this opportunity to convert text to lower-case\n",
    "    to avoid having to do this later on.\n",
    "    '''\n",
    "    doc = nlp(text)\n",
    "    spans = spacy.util.filter_spans(\n",
    "        set(doc.ents).union(set(doc.noun_chunks)))\n",
    "    entities = [t.lower_ for t in [span for span in spans]]\n",
    "    return entities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Identify unique entities across the corpus, and build a numeric index of each document and entity.\n",
    "The document and entitity indices will be used to reference values in a sparse matrix in a later step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Opts:\n",
    "    def __init__(self, init_from=None):\n",
    "        if init_from:\n",
    "            self.docs_m = init_from.docs_m\n",
    "            self.spans_n = init_from.spans_n\n",
    "            self.doc_index_m = init_from.doc_index_m\n",
    "            self.span_index_n = init_from.span_index_n\n",
    "        else:\n",
    "            self.docs_m = dict()\n",
    "            self.spans_n = dict()\n",
    "            self.doc_index_m = -1\n",
    "            self.span_index_n = -1\n",
    "        \n",
    "    def __str__(self):\n",
    "        return \"doc_index_m: {}, span_index_n: {}\".format(self.doc_index_m, self.span_index_n)\n",
    "    \n",
    "    def dump(self, file_name):\n",
    "        with open(file_name, 'bw') as f:\n",
    "            Pickler(f).dump(self)\n",
    "            \n",
    "    def load(self, file_name):\n",
    "        with open(file_name, 'br') as f:\n",
    "            self = Unpickler(f).load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opts = Opts()\n",
    "\n",
    "def inspect_file(file_name, o):\n",
    "    with open(os.path.join(base_path, file_name), 'r') as f:\n",
    "        text = f.read()\n",
    "    if not file_name in o.docs_m:\n",
    "        o.doc_index_m = o.doc_index_m + 1\n",
    "        o.docs_m[file_name] = o.doc_index_m\n",
    "    entities = extract_entities(text)\n",
    "    for entity in entities:\n",
    "        if not entity in o.spans_n:\n",
    "            o.span_index_n = o.span_index_n + 1\n",
    "            o.spans_n[entity] = o.span_index_n "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "futures = []\n",
    "with ThreadPoolExecutor(max_workers=MAX_WORKERS) as e:\n",
    "    for file_name in file_names[:DOC_LIMIT]:\n",
    "        futures.append(e.submit(inspect_file, file_name, opts))\n",
    "done, not_done = wait(futures, return_when=\"FIRST_EXCEPTION\")\n",
    "for d in done:\n",
    "    if d.exception():\n",
    "        [n.cancel() for n in not_done]\n",
    "        raise d.exception()\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "opts_file_name = os.path.join(os.getcwd(), \"research/data/doc_entity_indices.pickle\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opts = Opts(init_from=opts) # make sure we're dumping whatever is currently in the repl's memory.\n",
    "opts.dump(opts_file_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pass through all of the documents a second time.\n",
    "This time tabulating the span (entity) frequency per document for all known entities across the corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opts.load(opts_file_name)\n",
    "\n",
    "M = sparse.lil_matrix((len(opts.docs_m), len(opts.spans_n)))\n",
    "def analyze_corpus(file_name, matrix, o):\n",
    "    doc_index = o.docs_m[file_name]\n",
    "    with open(os.path.join(base_path, file_name), 'r') as f:\n",
    "        text = f.read().lower()\n",
    "    for span, span_index in o.spans_n.items():\n",
    "        matrix[doc_index, span_index] = text.count(span)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "futures = []\n",
    "with ThreadPoolExecutor(max_workers=MAX_WORKERS) as e:\n",
    "    for file_name in file_names[:DOC_LIMIT]:\n",
    "        futures.append(e.submit(analyze_corpus, file_name, M, opts))\n",
    "done, not_done = wait(futures, return_when=\"FIRST_EXCEPTION\")\n",
    "for d in done:\n",
    "    if d.exception():\n",
    "        [n.cancel() for n in not_done]\n",
    "        raise d.exception()\n",
    "print(\"done\")   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "npz_file_name = os.path.join(os.getcwd(), \"research/data/corpus_entity_matrix.npz\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sparse.save_npz(npz_file_name), M.tocsr())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Checking the density of the matrix for sanity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "def density(M):\n",
    "    d = 100 * M.nnz / np.prod(M.shape)\n",
    "    print(\"Matrix density: {}%\".format(round(d,2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matrix density: 1.28%\n"
     ]
    }
   ],
   "source": [
    "density(M)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate tfidf values within the matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "M = sparse.load_npz(npz_file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(500, 63271)"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "M.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "AXIS_DOCS = 0 # rows\n",
    "AXIS_ENTS = 1 # columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tfidf(M):\n",
    "    tf = sparse.csr_matrix(M / M.sum(axis=AXIS_ENTS))\n",
    "    N = M.shape[AXIS_DOCS]\n",
    "    Nt = np.ravel(M.astype(bool).sum(axis=AXIS_DOCS))\n",
    "    idf = sparse.csr_matrix(np.log10(N/Nt))\n",
    "    return tf.multiply(idf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/joe/Documents/code/rallymomentumindex/virtualenv/lib/python3.7/site-packages/scipy/sparse/base.py:599: RuntimeWarning: invalid value encountered in true_divide\n",
      "  return np.true_divide(self.todense(), other)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matrix density: 1.88%\n",
      "  (0, 62605)\t6.78082746157581e-05\n",
      "  (0, 62388)\t0.00012057166043859091\n",
      "  (0, 61564)\t6.33254851920391e-05\n",
      "  (0, 61392)\t0.000430852176695942\n",
      "  (0, 61270)\t0.00025546199680175226\n",
      "  (0, 61211)\t0.00031891741038330063\n",
      "  (0, 61139)\t0.00012749133060802967\n",
      "  (0, 61134)\t4.023624894882511e-05\n",
      "  (0, 60875)\t0.000215426088347971\n",
      "  (0, 60755)\t0.0001023368407786555\n",
      "  (0, 60617)\t4.04998435310284e-05\n",
      "  (0, 60404)\t8.291302605978031e-05\n",
      "  (0, 60373)\t0.0002210745475872741\n",
      "  (0, 59882)\t6.412770517409933e-05\n",
      "  (0, 59376)\t4.895986904193222e-05\n",
      "  (0, 59234)\t0.0002954979052555335\n",
      "  (0, 58645)\t3.447163046175633e-05\n",
      "  (0, 58637)\t4.0690954603795234e-05\n",
      "  (0, 58411)\t1.7380074495854882e-05\n",
      "  (0, 58111)\t0.00011992251402139868\n",
      "  (0, 58110)\t8.342435758010343e-05\n",
      "  (0, 57751)\t2.1843259557225113e-05\n",
      "  (0, 57451)\t4.45514263316302e-05\n",
      "  (0, 57416)\t9.531836298662726e-05\n",
      "  (0, 57285)\t0.0002659928181939088\n",
      "  :\t:\n",
      "  (499, 201)\t5.4591378080075324e-05\n",
      "  (499, 197)\t4.890036014954087e-05\n",
      "  (499, 192)\t0.00014335120326158031\n",
      "  (499, 189)\t8.036923869014351e-05\n",
      "  (499, 188)\t3.008728298814226e-05\n",
      "  (499, 185)\t8.406092120669584e-05\n",
      "  (499, 176)\t5.631593896228324e-06\n",
      "  (499, 158)\t5.6688088569799644e-05\n",
      "  (499, 153)\t0.0001049131986316755\n",
      "  (499, 149)\t3.4971728888320436e-05\n",
      "  (499, 147)\t2.6339011041646628e-05\n",
      "  (499, 145)\t5.59926403289019e-05\n",
      "  (499, 143)\t8.18679326750534e-05\n",
      "  (499, 126)\t2.091219661939548e-05\n",
      "  (499, 106)\t1.5261885158826703e-05\n",
      "  (499, 94)\t1.8098476500174443e-05\n",
      "  (499, 76)\t1.515259647323414e-05\n",
      "  (499, 68)\t1.9684291888652368e-05\n",
      "  (499, 60)\t0.0003284829898016528\n",
      "  (499, 53)\t4.645724229445414e-05\n",
      "  (499, 36)\t0.00014335120326158031\n",
      "  (499, 32)\t3.290182328739601e-05\n",
      "  (499, 13)\t0.00015883267513155046\n",
      "  (499, 11)\t6.747164103638798e-05\n",
      "  (499, 6)\t4.532777949875488e-05\n"
     ]
    }
   ],
   "source": [
    "M_tfidf = tfidf(M)\n",
    "density(M_tfidf)\n",
    "print(M_tfidf)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing tfidf function using data from https://en.wikipedia.org/wiki/Tf%E2%80%93idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 0)\t1\n",
      "  (0, 1)\t1\n",
      "  (0, 2)\t2\n",
      "  (0, 3)\t1\n",
      "  (1, 0)\t1\n",
      "  (1, 1)\t1\n",
      "  (1, 4)\t2\n",
      "  (1, 5)\t3\n"
     ]
    }
   ],
   "source": [
    "N = sparse.csr_matrix(np.array([[1,1,2,1,0,0],\n",
    "                                [1,1,0,0,2,3]]))\n",
    "print(N)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 3)\t0.06020599913279624\n",
      "  (0, 2)\t0.12041199826559248\n",
      "  (1, 5)\t0.12901285528456335\n",
      "  (1, 4)\t0.08600857018970891\n"
     ]
    }
   ],
   "source": [
    "print(tfidf(N))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
