{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BLM Keyword Identification\n",
    "Sandboxing some simple keyword identification routines.\n",
    "Full scale work will probably not be done in a notebook, mostly exploration here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import getcwd, listdir\n",
    "import os.path\n",
    "from concurrent.futures import ThreadPoolExecutor, wait\n",
    "\n",
    "import spacy\n",
    "from scipy import sparse\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_lg\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gather a list of codex entries to analyze."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "DOC_LIMIT = 1000\n",
    "\n",
    "base_path = os.path.join(getcwd(), \"research/data/codex\")\n",
    "\n",
    "file_names = [f for f in listdir(base_path) if os.path.isfile(os.path.join(base_path, f)) and f.endswith(\".txt\")]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Identifing an algorithm for named entity extraction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_entities(text):\n",
    "    '''\n",
    "    Identify entities and noun_chunks within the document,\n",
    "    and filter them such that there are no overlapping spans.\n",
    "    Also taking this opportunity to convert text to lower-case\n",
    "    to avoid having to do this later on.\n",
    "    '''\n",
    "    doc = nlp(text)\n",
    "    spans = spacy.util.filter_spans(\n",
    "        set(doc.ents).union(set(doc.noun_chunks)))\n",
    "    entities = [t.lower_ for t in [span for span in spans]]\n",
    "    return entities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Identify unique entities across the corpus, and build a numeric index of each document and entity.\n",
    "The document and entitity indices will be used to reference values in a sparse matrix in a later step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "opts = {\n",
    "    \"docs_m\" : dict(),\n",
    "    \"spans_n\" : dict(),\n",
    "    \"doc_index_m\" : -1,\n",
    "    \"span_index_n\" : -1\n",
    "}\n",
    "\n",
    "def inspect_file(file_name, o):\n",
    "    with open(os.path.join(base_path, file_name), 'r') as f:\n",
    "        text = f.read()\n",
    "    if not file_name in o[\"docs_m\"]:\n",
    "        o[\"doc_index_m\"] = o[\"doc_index_m\"] + 1\n",
    "        o[\"docs_m\"][file_name] = o[\"doc_index_m\"]\n",
    "    entities = extract_entities(text)\n",
    "    for entity in entities:\n",
    "        if not entity in o[\"spans_n\"]:\n",
    "            o[\"span_index_n\"] = o[\"span_index_n\"] + 1\n",
    "            o[\"spans_n\"][entity] = o[\"span_index_n\"]  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "futures = []\n",
    "with ThreadPoolExecutor(max_workers=25) as e:\n",
    "    for file_name in file_names[:DOC_LIMIT]:\n",
    "        futures.append(e.submit(inspect_file, file_name, opts))\n",
    "done, not_done = wait(futures)\n",
    "for d in done:\n",
    "    if d.exception():\n",
    "        raise d.exception()\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pass through all of the documents a second time.\n",
    "This time tabulating the span (entity) frequency per document for all known entities across the corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "M = sparse.lil_matrix((len(opts[\"docs_m\"]), len(opts[\"spans_n\"])))\n",
    "for file_name in file_names[:DOC_LIMIT]:\n",
    "    doc_index = opts[\"docs_m\"][file_name]\n",
    "    with open(os.path.join(base_path, file_name), 'r') as f:\n",
    "        text = f.read().lower()\n",
    "    for span, span_index in opts[\"spans_n\"].items():\n",
    "        M[doc_index, span_index] = text.count(span)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Checking the density of the matrix for sanity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matrix density: 14.9%\n"
     ]
    }
   ],
   "source": [
    "density = 100 * M.nnz / np.prod(M.shape)\n",
    "print(\"Matrix density: {}%\".format(round(density,2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate tfidf values within the matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
